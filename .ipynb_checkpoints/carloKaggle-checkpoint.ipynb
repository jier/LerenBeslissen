{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#reads in the data\n",
    "data = pd.read_csv('movie_metadata.csv')\n",
    "\n",
    "#removes the values that aren't numeric\n",
    "to_drop = ['director_name', 'num_critic_for_reviews', 'actor_2_name','actor_1_name', 'movie_title', 'num_voted_users'\n",
    "           , 'actor_3_name', 'plot_keywords', 'movie_imdb_link', 'num_user_for_reviews']\n",
    "\n",
    "#makes the new data set without the to_drop colums\n",
    "features_list = data.columns.difference(to_drop)\n",
    "movie_data = data[features_list]\n",
    "#print(np.sum(movie_num.isnull()))\n",
    "#print(movie_data.content_rating.unique())\n",
    "pd.options.mode.chained_assignment = None \n",
    "   \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Genres into individual Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Animation', 'Drama', 'Sport', 'Crime', 'Action', 'Family', 'Romance', 'Game-Show', 'History', 'Thriller', 'Sci-Fi', 'Horror', 'War', 'Musical', 'Mystery', 'Adventure', 'News', 'Fantasy', 'Reality-TV', 'Film-Noir', 'Biography', 'Documentary', 'Comedy', 'Short', 'Western', 'Music'}\n"
     ]
    }
   ],
   "source": [
    "# make a set with all unique genres\n",
    "\n",
    "genres = []\n",
    "\n",
    "for string in movie_data['genres']:\n",
    "    genre = string.split('|')\n",
    "    genres = genres+genre\n",
    "    \n",
    "genres_set = set(genres)\n",
    "print(genres_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "genres_dict = dict()\n",
    "for genre in genres_set:\n",
    "    genres_dict[genre] = []\n",
    "    \n",
    "for string in movie_data.genres:\n",
    "    genres = string.split('|')\n",
    "    for genre in genres_set:\n",
    "        if genre in genres:\n",
    "            genres_dict[genre] = genres_dict[genre]+[1]\n",
    "        else:\n",
    "            genres_dict[genre] = genres_dict[genre]+[0]\n",
    "\n",
    "del movie_data['genres']\n",
    "#print(genres_dict['Short'])\n",
    "\n",
    "for genre in genres_set:\n",
    "    series = pd.Series(genres_dict[genre])\n",
    "    movie_data[genre] = series\n",
    "\n",
    "#print(movie_data['Short'].values)    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before deletion: 5043\n",
      "Length after deletion: 3771\n",
      "Length before deletion: 5043\n",
      "Length after deletion: 4490\n",
      "Length before deletion: 5043\n",
      "Length after deletion: 4777\n",
      "Length before deletion: 5043\n",
      "Length after deletion: 4900\n"
     ]
    }
   ],
   "source": [
    "NA_THRESH = 4\n",
    "\n",
    "#deletes the row when there are more or equal to the threshod number of NaN's\n",
    "def remove_too_many_NaN(data, threshold):\n",
    "    print(\"Length before deletion: {}\".format(len(data)))\n",
    "\n",
    "    remove_indices = []\n",
    "    for index, nNaN in data.isnull().sum(axis=1).iteritems():\n",
    "        if nNaN >= threshold:\n",
    "            remove_indices.append(index)\n",
    "    \n",
    "    # drop movies with too many NaNs\n",
    "    data = data.drop(data.index[remove_indices])\n",
    "    print(\"Length after deletion: {}\".format(len(data)))\n",
    "    \n",
    "    return data\n",
    "\n",
    "movie_data_1 = movie_data.copy()\n",
    "movie_data_2 = movie_data.copy()\n",
    "movie_data_3 = movie_data.copy()\n",
    "movie_data_4 = movie_data.copy()\n",
    "\n",
    "movie_data_1 = remove_too_many_NaN(movie_data, 1)\n",
    "movie_data_2 = remove_too_many_NaN(movie_data, 2)\n",
    "movie_data_3 = remove_too_many_NaN(movie_data, 3)\n",
    "movie_data_4 = remove_too_many_NaN(movie_data, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Color to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def numeric_color(movie_data):\n",
    "    #replaces the color value to 1\n",
    "    movie_data.color = movie_data.color.replace(to_replace = 'Color', value = 1)\n",
    "    movie_data.color = movie_data.color.replace(to_replace = 'NaN', value = 1)\n",
    "\n",
    "    #replaces the NaN or black and white values to 0\n",
    "    for item in movie_data.color:\n",
    "        if item != 1:\n",
    "            movie_data.color = movie_data.color.replace(to_replace = item, value = 0)\n",
    "\n",
    "    #movie_data.color = new_color_column \n",
    "    #print(movie_data['color'])\n",
    "\n",
    "    #makes sure that there is nog error where it shouldn't be\n",
    "    pd.options.mode.chained_assignment= None\n",
    "    print('Done')\n",
    "    return movie_data\n",
    "\n",
    "movie_data_1 = numeric_color(movie_data_1)\n",
    "movie_data_2 = numeric_color(movie_data_2)\n",
    "movie_data_3 = numeric_color(movie_data_3)\n",
    "movie_data_4 = numeric_color(movie_data_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Country to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def numeric_country(movie_data):\n",
    "    #replaces the USA values to 1\n",
    "    movie_data.country = movie_data.country.replace(to_replace ='USA', value = 1)\n",
    "    movie_data.country = movie_data.country.replace(to_replace ='NaN', value = 1)\n",
    "    #replaces the NaN or non USA values to 0\n",
    "    for item in movie_data.country:\n",
    "        if item != 1:\n",
    "            movie_data.country = movie_data.country.replace(to_replace = item, value = 0)\n",
    "\n",
    "    #print(movie_data['country'])\n",
    "\n",
    "    #makes sure that there is nog error where it shouldn't be\n",
    "    pd.options.mode.chained_assignment= None\n",
    "\n",
    "    print('Done')\n",
    "    return movie_data\n",
    "\n",
    "movie_data_1 = numeric_country(movie_data_1)\n",
    "movie_data_2 = numeric_country(movie_data_2)\n",
    "movie_data_3 = numeric_country(movie_data_3)\n",
    "movie_data_4 = numeric_country(movie_data_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Languange to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def numeric_language(movie_data):\n",
    "    #replaces the English values to 1\n",
    "    movie_data.language = movie_data.language.replace(to_replace = 'English', value = 1)\n",
    "    movie_data.language = movie_data.language.replace(to_replace = 'NaN', value = 1)\n",
    "\n",
    "    #replaces the other values to 0\n",
    "    for item in movie_data.language:\n",
    "        if item != 1:\n",
    "            movie_data.language = movie_data.language.replace(to_replace = item, value = 0)\n",
    "\n",
    "    #print(movie_data['language'])\n",
    "\n",
    "    #makes sure that there is nog error where it shouldn't be\n",
    "    pd.options.mode.chained_assignment= None\n",
    "    print('Done')\n",
    "    return movie_data\n",
    "\n",
    "movie_data_1 = numeric_language(movie_data_1)\n",
    "movie_data_2 = numeric_language(movie_data_2)\n",
    "movie_data_3 = numeric_language(movie_data_3)\n",
    "movie_data_4 = numeric_language(movie_data_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaning Content_Rating to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def content_to_numerical(data):\n",
    "    data = data.replace(to_replace='G', value=0)\n",
    "    data = data.replace(to_replace='PG', value=12)\n",
    "    data = data.replace(to_replace='PG-13', value=13)\n",
    "    data = data.replace(to_replace='R', value=17)\n",
    "    data = data.replace(to_replace='NC-17', value=17)\n",
    "    \n",
    "    data = data.replace(to_replace='TV-PG', value=12)\n",
    "    data = data.replace(to_replace='TV-MA', value=17)\n",
    "    data = data.replace(to_replace='TV-G', value=0)\n",
    "    data = data.replace(to_replace='TV-Y', value=0)\n",
    "    data = data.replace(to_replace='TV-Y7', value=7)\n",
    "    data = data.replace(to_replace='TV-14', value=14)\n",
    "    \n",
    "    data = data.replace(to_replace='Not Rated', value=0)\n",
    "    data = data.replace(to_replace='Unrated', value=0)\n",
    "    data = data.replace(to_replace='Approved', value=0)\n",
    "    data = data.replace(to_replace='Passed', value=0)\n",
    "    \n",
    "    data = data.replace(to_replace='X', value=17)\n",
    "    data = data.replace(to_replace='M', value=17)\n",
    "    data = data.replace(to_replace='GP', value=12)\n",
    "    \n",
    "    return data\n",
    "\n",
    "movie_data_1 = content_to_numerical(movie_data_1)\n",
    "movie_data_2 = content_to_numerical(movie_data_2)\n",
    "movie_data_3 = content_to_numerical(movie_data_3)\n",
    "movie_data_4 = content_to_numerical(movie_data_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing NaNs with averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7722.74303898\n",
      "7032.59888641\n",
      "6747.99685995\n",
      "6648.62979592\n",
      "2014.86157518\n",
      "1792.34922049\n",
      "1713.49706868\n",
      "1684.1759902\n",
      "768.95332803\n",
      "691.49108337\n",
      "664.698322851\n",
      "653.779595175\n",
      "2.11111376293\n",
      "2.1096122633\n",
      "2.11774347826\n",
      "2.1284344634\n",
      "46074665.6696\n",
      "41840819.2828\n",
      "40444270.7169\n",
      "39892924.8304\n",
      "11486.9586317\n",
      "10423.8097996\n",
      "10003.1101109\n",
      "9849.74408163\n",
      "804.246353752\n",
      "744.864587973\n",
      "703.776847394\n",
      "692.662510221\n",
      "52410371.7385\n",
      "49287524.5918\n",
      "48619441.8755\n",
      "48514997.6517\n",
      "2003.01060727\n",
      "2001.97906459\n",
      "2002.19866025\n",
      "2002.39901881\n",
      "110.219570406\n",
      "109.24142539\n",
      "108.555276382\n",
      "108.243568804\n",
      "1.37708830549\n",
      "1.35540691193\n",
      "1.36479966436\n",
      "1.36272504092\n",
      "14.1148236542\n",
      "13.7810398924\n",
      "13.6992675571\n",
      "13.6882617062\n"
     ]
    }
   ],
   "source": [
    "def replace_NaNs(col):\n",
    "    # compute average\n",
    "    avg = np.sum(col) / (len(col) - np.sum(col.isnull()))\n",
    "    print(avg)\n",
    "    \n",
    "    # replace NaNs with average\n",
    "    col = col.fillna(value=avg)\n",
    "    return col\n",
    "\n",
    "pd.options.mode.chained_assignment= None\n",
    "\n",
    "to_replace_NaNs = ['actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes', \n",
    "               'aspect_ratio', 'budget', 'cast_total_facebook_likes', 'director_facebook_likes','gross', 'title_year'\n",
    "                   , 'duration', 'facenumber_in_poster', 'content_rating']\n",
    "\n",
    "for column in to_replace_NaNs:\n",
    "    movie_data_1[column] = replace_NaNs(movie_data_1[column])\n",
    "    movie_data_2[column] = replace_NaNs(movie_data_2[column])\n",
    "    movie_data_3[column] = replace_NaNs(movie_data_3[column])\n",
    "    movie_data_4[column] = replace_NaNs(movie_data_4[column])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Delete columns with too little 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def del_sparse_cols(movie_data, doprint=False):\n",
    "    for genre in genres_set:\n",
    "        if doprint:\n",
    "            print(genre, ':')\n",
    "        counter = 0\n",
    "        for value in movie_data[genre]:\n",
    "            if value == 1.0:\n",
    "                counter+=1\n",
    "        \n",
    "        if doprint:\n",
    "            print(counter)\n",
    "\n",
    "    weghalen = ['Game-Show', 'News', 'Reality-TV', 'Short', 'Film-Noir']\n",
    "\n",
    "    for name in weghalen:\n",
    "        del movie_data[name]\n",
    "\n",
    "    #print(movie_data)\n",
    "    return movie_data\n",
    "\n",
    "movie_data_1 = del_sparse_cols(movie_data_1)\n",
    "movie_data_2 = del_sparse_cols(movie_data_2)\n",
    "movie_data_3 = del_sparse_cols(movie_data_3)\n",
    "movie_data_4 = del_sparse_cols(movie_data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cor(data):\n",
    "    plt.figure(figsize=(10,10), tight_layout=True)\n",
    "    plt.pcolor(data.corr())\n",
    "    plt.yticks(np.arange(0.5, len(list(data)), 1), list(data))\n",
    "    plt.xticks(np.arange(0.5, len(list(data)), 1), list(data), rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "#plot_cor(movie_data_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainset en testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263\n",
      "754\n",
      "754\n",
      "2694\n",
      "898\n",
      "898\n",
      "2866\n",
      "955\n",
      "956\n",
      "2940\n",
      "980\n",
      "980\n"
     ]
    }
   ],
   "source": [
    "def split_to_sets(movie_data):\n",
    "    movie_data_sets = {'movie_training': 0, 'ratings_training': 0, \n",
    "                       'movie_validation': 0, 'ratings_validation': 0, \n",
    "                       'movie_test': 0, 'ratings_test': 0}\n",
    "    \n",
    "    ratings = movie_data['imdb_score'].values\n",
    "    del movie_data['imdb_score']\n",
    "\n",
    "    X = movie_data.values\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "    number_of_samples = len(ratings)\n",
    "    np.random.seed(0)\n",
    "    random_indices = np.random.permutation(number_of_samples)\n",
    "    num_training_samples = round(number_of_samples*0.6)\n",
    "    num_validation_samples = round(number_of_samples*0.2)+num_training_samples\n",
    "\n",
    "    movie_data_sets['movie_training'] = X_std[random_indices[:num_training_samples]]\n",
    "    ratings_training = ratings[random_indices[:num_training_samples]]\n",
    "\n",
    "    movie_data_sets['movie_validation'] = X_std[random_indices[num_training_samples:num_validation_samples]]\n",
    "    movie_data_sets['ratings_validation'] = ratings[random_indices[num_training_samples:num_validation_samples]]\n",
    "\n",
    "    movie_data_sets['movie_test'] = X_std[random_indices[num_validation_samples:]]\n",
    "    movie_data_sets['ratings_test'] = ratings[random_indices[num_validation_samples:]]\n",
    "\n",
    "    movie_data_sets['ratings_training'] = list(ratings_training)\n",
    "\n",
    "    print(len(movie_data_sets['ratings_training']))\n",
    "    print(len(movie_data_sets['ratings_validation']))\n",
    "    print(len(movie_data_sets['ratings_test']))\n",
    "    \n",
    "    return movie_data_sets\n",
    "\n",
    "sets = []\n",
    "sets.append(split_to_sets(movie_data_1))\n",
    "sets.append(split_to_sets(movie_data_2))\n",
    "sets.append(split_to_sets(movie_data_3))\n",
    "sets.append(split_to_sets(movie_data_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relative_error(y_predict, y):\n",
    "    \n",
    "    error = 0\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        error += (abs(y_predict[i]-y[i]))/y[i]\n",
    "    training_error = error/len(y)*100\n",
    "    \n",
    "    return training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyse_predictions(regr, sets, statdict, doprint=False):\n",
    "    x_test = sets['movie_test']\n",
    "    y_test = sets['ratings_test']\n",
    "    \n",
    "    regr.fit (sets['movie_training'], sets['ratings_training'])\n",
    "\n",
    "    prediction = regr.predict(x_test)\n",
    "    results_list.append(prediction)\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % np.mean((prediction - y_test) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % regr.score(x_test, y_test))\n",
    "\n",
    "    \n",
    "    \n",
    "    predres = list(regr.predict(x_test))\n",
    "    actual = list(y_test)\n",
    "    highest = avg = error = 0\n",
    "    lowest = predres[0]\n",
    "    for i in range(len(predres)):\n",
    "        diff = abs(predres[i] - actual[i])\n",
    "        avg += diff\n",
    "        error += abs(predres[i] - actual[i]) / actual[i]\n",
    "        \n",
    "        if doprint:\n",
    "            print(\"predicted:\", predres[i]) \n",
    "            print (\"\\tActual:\", actual[i])\n",
    "            print (\"\\tDifference:\", abs(predres[i] - actual[i]))\n",
    "\n",
    "        if diff > highest:\n",
    "            highest = diff\n",
    "        if diff < lowest:\n",
    "            lowest = diff\n",
    "\n",
    "    errper = error / len(x_test) * 100\n",
    "    avgdif =  avg / len(predres)\n",
    "    \n",
    "    if doprint:\n",
    "        #print \"Number of Samples:\", len(predres)\n",
    "        #print \"Highest difference:\", highest\n",
    "        #print \"Lowest difference:\", lowest\n",
    "        #print \"average difference:\", avgdif\n",
    "        print(\"Error Percentage:\", errper)\n",
    "    \n",
    "#     statdict['highest'].append(highest)\n",
    "#     statdict['lowest'].append(lowest)\n",
    "#     statdict['avgdif'].append(avgdif)\n",
    "    statdict['errper'].append(errper)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_list = []\n",
    "statdicts = []\n",
    "for i in range(4):\n",
    "    statdict = {'names':[], 'errper': [], 'avgdif': [], 'highest': [], 'lowest': [], 'predictions': []}\n",
    "    statdicts.append(statdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 8.299093859692332 percent in neural network algorithm\n",
      "Test error = 9.836597018707897 percent in neural network algorithm\n",
      "Train error = 8.836540395985878 percent in neural network algorithm\n",
      "Test error = 10.728500585657429 percent in neural network algorithm\n",
      "Train error = 10.687027307058674 percent in neural network algorithm\n",
      "Test error = 12.566443550420809 percent in neural network algorithm\n",
      "Train error = 7.564529533405169 percent in neural network algorithm\n",
      "Test error = 10.998149432880453 percent in neural network algorithm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def neural(statdict, sets):\n",
    "\n",
    "    neural = MLPRegressor(hidden_layer_sizes =(100), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "\n",
    "    neural.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_neural_train = neural.predict(sets['movie_training'])\n",
    "    y_neural_test = neural.predict(sets['movie_test'])\n",
    "\n",
    "    training_error = relative_error(y_neural_train, sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(training_error)+\" percent\"+\" in neural network algorithm\")\n",
    "\n",
    "    test_error = relative_error(y_neural_test,sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in neural network algorithm\")\n",
    "    \n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('neural')\n",
    "    statdict['predictions'].append(y_neural_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(4):\n",
    "    statdicts[i] = neural(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 11.049064079622106 percent in knn algorithm\n",
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 11.769553886564413 percent in knn algorithm\n",
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 13.31806282108924 percent in knn algorithm\n",
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 11.878901778400039 percent in knn algorithm\n"
     ]
    }
   ],
   "source": [
    "def do_knn(statdict, sets, n_neighbors=5, weights='uniform'):\n",
    "        \n",
    "    neighbors = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights)\n",
    "    neighbors.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_neighbors_train = neighbors.predict(sets['movie_training'])\n",
    "    #y_predict_train = list(y_predict_train)\n",
    "\n",
    "    train_error = relative_error(y_neighbors_train,sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in knn algorithm\")\n",
    "\n",
    "    y_neighbors_test = neighbors.predict(sets['movie_test'])\n",
    "    #y_predict_test = list(y_predict_test)\n",
    "\n",
    "    test_error = relative_error(y_neighbors_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in knn algorithm\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('kNN')\n",
    "    statdict['predictions'].append(y_neighbors_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(4):\n",
    "    statdicts[i] = do_knn(statdicts[i], sets[i], n_neighbors=15, weights='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 8.187619393138535\n",
      "Test error = 10.250489491713079\n",
      "Train error = 8.615541896633948\n",
      "Test error = 10.91478683155302\n",
      "Train error = 8.697265271348568\n",
      "Test error = 12.180230344638748\n",
      "Train error = 9.150128853631955\n",
      "Test error = 10.858612788391126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR, NuSVR, LinearSVR\n",
    "\n",
    "def do_svr(statdict, sets):\n",
    "    svr = SVR()\n",
    "    svr.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_svr_train = svr.predict(sets['movie_training'])\n",
    "    #y_train = list(y_train)\n",
    "\n",
    "    train_error = relative_error(y_svr_train, sets['ratings_training'])\n",
    "\n",
    "    print (\"Train error = \"+'{}'.format(train_error))\n",
    "\n",
    "    y_svr_test = svr.predict(sets['movie_test'])\n",
    "    #y_test = list(y_test)\n",
    "\n",
    "    test_error = relative_error(y_svr_test, sets['ratings_test'])\n",
    "\n",
    "    print (\"Test error = \"+'{}'.format(test_error))\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('SVM')\n",
    "    statdict['predictions'].append(y_svr_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(4):\n",
    "    statdicts[i] = do_svr(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 4.749036234303192e-16 percent in decision trees\n",
      "Test error = 12.594576574137022 percent in decision trees\n",
      "Train error = 3.8335588854045244e-16 percent in decision trees\n",
      "Test error = 13.019590560866554 percent in decision trees\n",
      "Train error = 3.859605849767512e-16 percent in decision trees\n",
      "Test error = 15.274257462941554 percent in decision trees\n",
      "Train error = 3.464430015449405e-16 percent in decision trees\n",
      "Test error = 14.62629791711067 percent in decision trees\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def do_dt(statdict, sets):\n",
    "    trees = DecisionTreeRegressor()\n",
    "    trees.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_trees_train = trees.predict(sets['movie_training'])\n",
    "\n",
    "    train_error = relative_error(y_trees_train, sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in decision trees\")\n",
    "\n",
    "    y_trees_test = trees.predict(sets['movie_test'])\n",
    "\n",
    "    test_error = relative_error(y_trees_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in decision trees\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('DT')\n",
    "    statdict['predictions'].append(y_trees_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(4):\n",
    "    statdicts[i] = do_dt(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 3.8075889505240545 percent in random forest\n",
      "Test error = 9.483932555437839 percent in random forest\n",
      "Train error = 3.8133884867744183 percent in random forest\n",
      "Test error = 10.483396158004812 percent in random forest\n",
      "Train error = 3.845012462176091 percent in random forest\n",
      "Test error = 11.259504979854526 percent in random forest\n",
      "Train error = 4.084538519059716 percent in random forest\n",
      "Test error = 10.575777785653967 percent in random forest\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "\n",
    "def do_randforest(statdict, sets):\n",
    "    random_forest = RandomForestRegressor(n_estimators = 20)\n",
    "    random_forest.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_forest_train = random_forest.predict(sets['movie_training'])\n",
    "\n",
    "    train_error = relative_error(y_forest_train, sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    y_forest_test = random_forest.predict(sets['movie_test'])\n",
    "\n",
    "    test_error = relative_error(y_forest_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('Rfor')\n",
    "    statdict['predictions'].append(y_forest_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(4):\n",
    "    statdicts[i] = do_randforest(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely randomized trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 1.7886980415022033e-14 percent in random forest\n",
      "Test error = 9.879029757442733 percent in random forest\n",
      "Train error = 1.7894243174509108e-14 percent in random forest\n",
      "Test error = 10.482494151259939 percent in random forest\n",
      "Train error = 1.7990798878596814e-14 percent in random forest\n",
      "Test error = 11.76399273598605 percent in random forest\n",
      "Train error = 1.821391130462263e-14 percent in random forest\n",
      "Test error = 10.573558399131432 percent in random forest\n"
     ]
    }
   ],
   "source": [
    "def do_xrandtrees(statdict, sets):\n",
    "    extra = ExtraTreesRegressor(n_estimators = 20)\n",
    "    extra.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_extra_train = extra.predict(sets['movie_training'])\n",
    "\n",
    "    train_error = relative_error(y_extra_train, sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    y_extra_test = extra.predict(sets['movie_test'])\n",
    "\n",
    "    test_error = relative_error(y_extra_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('XRfor')\n",
    "    statdict['predictions'].append(y_extra_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(4):\n",
    "    statdicts[i] = do_xrandtrees(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_baggingregr(statdict, sets):\n",
    "    #regressor  = BaggingRegressor(MLPRegressor(max_iter = 300))\n",
    "    bagging = BaggingRegressor(RandomForestRegressor())\n",
    "    bagging.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_bagging_train = bagging.predict(sets['movie_training'])\n",
    "    #y_predict_train = list(y_predict_train)\n",
    "\n",
    "    train_error = relative_error(y_bagging_train ,sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    y_bagging_test = bagging.predict(sets['movie_test'])\n",
    "    #y_predict_test = list(y_predict_test)\n",
    "\n",
    "    test_error = relative_error(y_bagging_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('bag')\n",
    "    statdict['predictions'].append(y_bagging_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "# for i in range(4):\n",
    "#     statdicts[i] = do_baggingregr(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neural_1 = MLPRegressor(hidden_layer_sizes =(100), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "# neural_2 = MLPRegressor(hidden_layer_sizes =(50), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "# neural_3 = MLPRegressor(hidden_layer_sizes =(10), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "# neural_4 = MLPRegressor(hidden_layer_sizes =(200), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "\n",
    "# neural_1.fit(movie_training, ratings_training)\n",
    "# neural_2.fit(movie_training, ratings_training)\n",
    "# neural_3.fit(movie_training, ratings_training)\n",
    "# neural_4.fit(movie_training, ratings_training)\n",
    "\n",
    "# y_1_neural = neural_1.predict(movie_validation)\n",
    "# y_2_neural = neural_2.predict(movie_validation)\n",
    "# y_3_neural = neural_3.predict(movie_validation)\n",
    "# y_4_neural = neural_4.predict(movie_validation)\n",
    "\n",
    "# error_1_neural = relative_error(y_1_neural, ratings_validation)\n",
    "# error_2_neural = relative_error(y_2_neural, ratings_validation)\n",
    "# error_3_neural = relative_error(y_3_neural, ratings_validation)\n",
    "# error_4_neural = relative_error(y_4_neural, ratings_validation)\n",
    "\n",
    "# print(error_1_neural)\n",
    "# print(error_2_neural)\n",
    "# print(error_3_neural)\n",
    "# print(error_4_neural)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# de parameters die kunnen worden aangepast \n",
    "\n",
    "# svr_1 = SVR(C = 0.0001)\n",
    "# svr_2 = SVR(C = 0.01)\n",
    "# svr_3 = SVR(C = 1.0)\n",
    "# svr_4 = SVR(C = 100)\n",
    "# svr_5 = SVR(C = 10000)\n",
    "\n",
    "# svr_1 = SVR(epsilon = 0.001)\n",
    "# svr_2 = SVR(epsilon = 0.01)\n",
    "# svr_3 = SVR(epsilon = 0.1)\n",
    "# svr_4 = SVR(epsilon = 10)\n",
    "# svr_5 = SVR(epsilon = 100)\n",
    "\n",
    "# svr_1.fit(movie_training, ratings_training)\n",
    "# svr_2.fit(movie_training, ratings_training)\n",
    "# svr_3.fit(movie_training, ratings_training)\n",
    "# svr_4.fit(movie_training, ratings_training)\n",
    "# svr_5.fit(movie_training, ratings_training)\n",
    "\n",
    "# y_1_svr = svr_1.predict(movie_validation)\n",
    "# y_2_svr = svr_2.predict(movie_validation)\n",
    "# y_3_svr = svr_3.predict(movie_validation)\n",
    "# y_4_svr = svr_4.predict(movie_validation)\n",
    "# y_5_svr = svr_5.predict(movie_validation)\n",
    "\n",
    "# error_1_svr = relative_error(y_1_svr, ratings_validation)\n",
    "# error_2_svr = relative_error(y_2_svr, ratings_validation)\n",
    "# error_3_svr = relative_error(y_3_svr, ratings_validation)\n",
    "# error_4_svr = relative_error(y_4_svr, ratings_validation)\n",
    "# error_5_svr = relative_error(y_5_svr, ratings_validation)\n",
    "\n",
    "# print(error_1_svr)\n",
    "# print(error_2_svr)\n",
    "# print(error_3_svr)\n",
    "# print(error_4_svr)\n",
    "# print(error_5_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# forest_1 = RandomForestRegressor(n_estimators = 5)\n",
    "# forest_2 = RandomForestRegressor(n_estimators = 10)\n",
    "# forest_3 = RandomForestRegressor(n_estimators = 20)\n",
    "# forest_4 = RandomForestRegressor(n_estimators = 40)\n",
    "# forest_5 = RandomForestRegressor(n_estimators = 80)\n",
    "\n",
    "# forest_1.fit(movie_training, ratings_training)\n",
    "# forest_2.fit(movie_training, ratings_training)\n",
    "# forest_3.fit(movie_training, ratings_training)\n",
    "# forest_4.fit(movie_training, ratings_training)\n",
    "# forest_5.fit(movie_training, ratings_training)\n",
    "\n",
    "# y_1_forest = forest_1.predict(movie_validation)\n",
    "# y_2_forest = forest_2.predict(movie_validation)\n",
    "# y_3_forest = forest_3.predict(movie_validation)\n",
    "# y_4_forest = forest_4.predict(movie_validation)\n",
    "# y_5_forest = forest_5.predict(movie_validation)\n",
    "\n",
    "# error_1_forest = relative_error(y_1_forest, ratings_validation)\n",
    "# error_2_forest = relative_error(y_2_forest, ratings_validation)\n",
    "# error_3_forest = relative_error(y_3_forest, ratings_validation)\n",
    "# error_4_forest = relative_error(y_4_forest, ratings_validation)\n",
    "# error_5_forest = relative_error(y_5_forest, ratings_validation)\n",
    "\n",
    "# print(error_1_forest)\n",
    "# print(error_2_forest)\n",
    "# print(error_3_forest)\n",
    "# print(error_4_forest)\n",
    "# print(error_5_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely randomized trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extra_1 = ExtraTreesRegressor(n_estimators = 5)\n",
    "# extra_2 = ExtraTreesRegressor(n_estimators = 10)\n",
    "# extra_3 = ExtraTreesRegressor(n_estimators = 20)\n",
    "# extra_4 = ExtraTreesRegressor(n_estimators = 40)\n",
    "# extra_5 = ExtraTreesRegressor(n_estimators = 80)\n",
    "\n",
    "# # extra_1 = ExtraTreesRegressor(n_estimators = 20, max_features = 'auto')\n",
    "# # extra_2 = ExtraTreesRegressor(n_estimators = 20, max_features = 'sqrt')\n",
    "# # extra_3 = ExtraTreesRegressor(n_estimators = 20, max_features = 'log2')\n",
    "# # extra_4 = ExtraTreesRegressor()\n",
    "# # extra_5 = ExtraTreesRegressor()\n",
    "\n",
    "# extra_1.fit(movie_training, ratings_training)\n",
    "# extra_2.fit(movie_training, ratings_training)\n",
    "# extra_3.fit(movie_training, ratings_training)\n",
    "# extra_4.fit(movie_training, ratings_training)\n",
    "# extra_5.fit(movie_training, ratings_training)\n",
    "\n",
    "# y_1_extra = extra_1.predict(movie_validation)\n",
    "# y_2_extra = extra_2.predict(movie_validation)\n",
    "# y_3_extra = extra_3.predict(movie_validation)\n",
    "# y_4_extra = extra_4.predict(movie_validation)\n",
    "# y_5_extra = extra_5.predict(movie_validation)\n",
    "\n",
    "# error_1_extra = relative_error(y_1_extra, ratings_validation)\n",
    "# error_2_extra = relative_error(y_2_extra, ratings_validation)\n",
    "# error_3_extra = relative_error(y_3_extra, ratings_validation)\n",
    "# error_4_extra = relative_error(y_4_extra, ratings_validation)\n",
    "# error_5_extra = relative_error(y_5_extra, ratings_validation)\n",
    "\n",
    "# print(error_1_extra)\n",
    "# print(error_2_extra)\n",
    "# print(error_3_extra)\n",
    "# print(error_4_extra)\n",
    "# print(error_5_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neighbor_1 = KNeighborsRegressor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trees_1 ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks and svr and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.22508460741\n",
      "0.09795918367346938\n",
      "10.0606737998\n",
      "0.12959183673469388\n",
      "11.2156097336\n",
      "0.1510204081632653\n",
      "10.116869678\n",
      "0.1673469387755102\n"
     ]
    }
   ],
   "source": [
    "def do_ensemble(statdict, sets):\n",
    "    y_svr_test = statdict['predictions'][statdict['names'].index('SVM')]\n",
    "    y_forest_test = statdict['predictions'][statdict['names'].index('Rfor')]\n",
    "    y_extra_test = statdict['predictions'][statdict['names'].index('XRfor')]\n",
    "    ratings_test = sets['ratings_test']\n",
    "\n",
    "\n",
    "    prediction = [sum(x)/3 for x in zip(y_svr_test, y_forest_test, y_extra_test)]\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(len(prediction)):\n",
    "        if abs(prediction[i]-ratings_test[i])>1.0:\n",
    "            counter+= 1\n",
    "            #print(prediction[i], ratings_test[i])\n",
    "\n",
    "    error_ensemble = relative_error(prediction, ratings_test)\n",
    "\n",
    "    statdict['errper'].append(error_ensemble)\n",
    "    statdict['names'].append('ensemble')\n",
    "    statdict['predictions'].append(prediction)\n",
    "\n",
    "    print(error_ensemble)\n",
    "    print(counter/980.0)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(4):\n",
    "    statdicts[i] = do_ensemble(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(movie_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# movie_data['color'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.69\n",
      "Variance score: 0.37\n",
      "Mean squared error: 0.69\n",
      "Variance score: 0.38\n",
      "Mean squared error: 0.68\n",
      "Variance score: 0.39\n",
      "Mean squared error: 0.67\n",
      "Variance score: 0.39\n",
      "Mean squared error: 0.72\n",
      "Variance score: 0.41\n",
      "Mean squared error: 0.72\n",
      "Variance score: 0.41\n",
      "Mean squared error: 0.72\n",
      "Variance score: 0.41\n",
      "Mean squared error: 0.72\n",
      "Variance score: 0.41\n",
      "Mean squared error: 0.87\n",
      "Variance score: 0.34\n",
      "Mean squared error: 0.87\n",
      "Variance score: 0.33\n",
      "Mean squared error: 0.88\n",
      "Variance score: 0.33\n",
      "Mean squared error: 0.88\n",
      "Variance score: 0.33\n",
      "Mean squared error: 0.77\n",
      "Variance score: 0.38\n",
      "Mean squared error: 0.77\n",
      "Variance score: 0.38\n",
      "Mean squared error: 0.78\n",
      "Variance score: 0.38\n",
      "Mean squared error: 0.78\n",
      "Variance score: 0.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def do_linear_models(statdict, sets):\n",
    "    regr = linear_model.LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "\n",
    "    statdict = analyse_predictions(regr, sets, statdict)\n",
    "    statdict['names'].append('linreg')\n",
    "\n",
    "\n",
    "    regr = linear_model.Ridge()\n",
    "    statdict = analyse_predictions(regr, sets, statdict)\n",
    "    statdict['names'].append('ridgereg')\n",
    "\n",
    "\n",
    "    regr = linear_model.Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000000,\n",
    "       normalize=False, positive=False, precompute=False, random_state=None,\n",
    "       selection='cyclic', tol=0.0001, warm_start=False)\n",
    "    statdict = analyse_predictions(regr, sets, statdict)\n",
    "    statdict['names'].append('lassoreg')\n",
    "\n",
    "\n",
    "    regr = linear_model.BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
    "           fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
    "           normalize=False, tol=0.001, verbose=False)\n",
    "    statdict = analyse_predictions(regr, sets, statdict)\n",
    "    statdict['names'].append('bayesian')\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    statdicts[i] = do_linear_models(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_threshold(statdicts):\n",
    "    n_groups = len(statdicts[0]['names'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5)\n",
    "    \n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.9 * (1.0 / 4)\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = plt.bar(index, statdicts[0]['errper'], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     error_kw=error_config,\n",
    "                     label=\"threshold 1\")\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, statdicts[1]['errper'], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='g',\n",
    "                     error_kw=error_config,\n",
    "                     label=\"threshold 2\")\n",
    "\n",
    "    rects3 = plt.bar(index + bar_width * 2, statdicts[2]['errper'], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='b',\n",
    "                     error_kw=error_config,\n",
    "                     label=\"threshold 3\")\n",
    "\n",
    "    rects4 = plt.bar(index + bar_width * 3, statdicts[3]['errper'], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='c',\n",
    "                     error_kw=error_config,\n",
    "                     label=\"threshold 4\")\n",
    "\n",
    "    plt.xlabel('Algorithm')\n",
    "    plt.ylabel('Error Percentages (in %)')\n",
    "    plt.title('Error Percentages for various algorithms depending on the threshold')\n",
    "    plt.xticks(index + bar_width * 2, statdict['names'], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.legend(bbox_to_anchor=(0, 0.3), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    \n",
    "plot_threshold(statdicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_errors(statdict):\n",
    "    n_groups = len(statdict['names'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.9\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = plt.bar(index, statdict['errper'], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     error_kw=error_config)\n",
    "\n",
    "    plt.xlabel('Algorithm')\n",
    "    plt.ylabel('Error Percentages (in %)')\n",
    "    plt.title('Error Percentages for various algorithms using threshold 4')\n",
    "    plt.xticks(index + bar_width * 0.5, statdict['names'], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_errors(statdicts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
