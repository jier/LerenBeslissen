{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['actor_1_facebook_likes', 'actor_2_facebook_likes',\n",
      "       'actor_3_facebook_likes', 'aspect_ratio', 'budget',\n",
      "       'cast_total_facebook_likes', 'color', 'content_rating', 'country',\n",
      "       'director_facebook_likes', 'duration', 'facenumber_in_poster', 'genres',\n",
      "       'gross', 'imdb_score', 'language', 'movie_facebook_likes',\n",
      "       'title_year'],\n",
      "      dtype='object')\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#reads in the data\n",
    "data = pd.read_csv('movie_metadata.csv')\n",
    "\n",
    "#removes the values that aren't numeric\n",
    "to_drop = ['director_name', 'num_critic_for_reviews', 'actor_2_name','actor_1_name', 'movie_title', 'num_voted_users'\n",
    "           , 'actor_3_name', 'plot_keywords', 'movie_imdb_link', 'num_user_for_reviews']\n",
    "\n",
    "#makes the new data set without the to_drop colums\n",
    "features_list = data.columns.difference(to_drop)\n",
    "movie_data = data[features_list]\n",
    "#print(np.sum(movie_num.isnull()))\n",
    "#print(movie_data.content_rating.unique())\n",
    "pd.options.mode.chained_assignment = None \n",
    "print(movie_data.columns)   \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Genres into individual Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sci-Fi', 'History', 'News', 'Crime', 'Family', 'Short', 'Reality-TV', 'Fantasy', 'Romance', 'Animation', 'Documentary', 'Film-Noir', 'Adventure', 'Musical', 'Sport', 'Western', 'Drama', 'Mystery', 'Thriller', 'Biography', 'Horror', 'Comedy', 'Action', 'War', 'Game-Show', 'Music'}\n"
     ]
    }
   ],
   "source": [
    "# make a set with all unique genres\n",
    "\n",
    "genres = []\n",
    "\n",
    "for string in movie_data['genres']:\n",
    "    genre = string.split('|')\n",
    "    genres = genres+genre\n",
    "    \n",
    "genres_set = set(genres)\n",
    "print(genres_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "genres_dict = dict()\n",
    "for genre in genres_set:\n",
    "    genres_dict[genre] = []\n",
    "    \n",
    "for string in movie_data.genres:\n",
    "    genres = string.split('|')\n",
    "    for genre in genres_set:\n",
    "        if genre in genres:\n",
    "            genres_dict[genre] = genres_dict[genre]+[1]\n",
    "        else:\n",
    "            genres_dict[genre] = genres_dict[genre]+[0]\n",
    "\n",
    "del movie_data['genres']\n",
    "#print(genres_dict['Short'])\n",
    "\n",
    "for genre in genres_set:\n",
    "    series = pd.Series(genres_dict[genre])\n",
    "    movie_data[genre] = series\n",
    "\n",
    "#print(movie_data['Short'].values)    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before deletion: 5043\n",
      "Length after deletion: 4900\n"
     ]
    }
   ],
   "source": [
    "NA_THRESH = 4\n",
    "\n",
    "#deletes the row when there are more or equal to the threshod number of NaN's\n",
    "def remove_too_many_NaN(data, threshold):\n",
    "    print(\"Length before deletion: {}\".format(len(data)))\n",
    "\n",
    "    remove_indices = []\n",
    "    for index, nNaN in data.isnull().sum(axis=1).iteritems():\n",
    "        if nNaN >= threshold:\n",
    "            remove_indices.append(index)\n",
    "    \n",
    "    # drop movies with too many NaNs\n",
    "    data = data.drop(data.index[remove_indices])\n",
    "    print(\"Length after deletion: {}\".format(len(data)))\n",
    "    \n",
    "    return data\n",
    "\n",
    "#returns length\n",
    "movie_data = remove_too_many_NaN(movie_data, NA_THRESH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Color to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#replaces the color value to 1\n",
    "movie_data.color = movie_data.color.replace(to_replace = 'Color', value = 1)\n",
    "movie_data.color = movie_data.color.replace(to_replace = 'NaN', value = 1)\n",
    "\n",
    "#replaces the NaN or black and white values to 0\n",
    "for item in movie_data.color:\n",
    "    if item != 1:\n",
    "        movie_data.color = movie_data.color.replace(to_replace = item, value = 0)\n",
    "        \n",
    "#movie_data.color = new_color_column \n",
    "#print(movie_data['color'])\n",
    "\n",
    "#makes sure that there is nog error where it shouldn't be\n",
    "pd.options.mode.chained_assignment= None\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Country to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#replaces the USA values to 1\n",
    "movie_data.country = movie_data.country.replace(to_replace ='USA', value = 1)\n",
    "movie_data.country = movie_data.country.replace(to_replace ='NaN', value = 1)\n",
    "#replaces the NaN or non USA values to 0\n",
    "for item in movie_data.country:\n",
    "    if item != 1:\n",
    "        movie_data.country = movie_data.country.replace(to_replace = item, value = 0)\n",
    "       \n",
    "#print(movie_data['country'])\n",
    "\n",
    "#makes sure that there is nog error where it shouldn't be\n",
    "pd.options.mode.chained_assignment= None\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Languange to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#replaces the English values to 1\n",
    "movie_data.language = movie_data.language.replace(to_replace = 'English', value = 1)\n",
    "movie_data.language = movie_data.language.replace(to_replace = 'NaN', value = 1)\n",
    "\n",
    "#replaces the other values to 0\n",
    "for item in movie_data.language:\n",
    "    if item != 1:\n",
    "        movie_data.language = movie_data.language.replace(to_replace = item, value = 0)\n",
    "        \n",
    "#print(movie_data['language'])\n",
    "\n",
    "#makes sure that there is nog error where it shouldn't be\n",
    "pd.options.mode.chained_assignment= None\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaning Content_Rating to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def content_to_numerical(data):\n",
    "    data = data.replace(to_replace='G', value=0)\n",
    "    data = data.replace(to_replace='PG', value=12)\n",
    "    data = data.replace(to_replace='PG-13', value=13)\n",
    "    data = data.replace(to_replace='R', value=17)\n",
    "    data = data.replace(to_replace='NC-17', value=17)\n",
    "    \n",
    "    data = data.replace(to_replace='TV-PG', value=12)\n",
    "    data = data.replace(to_replace='TV-MA', value=17)\n",
    "    data = data.replace(to_replace='TV-G', value=0)\n",
    "    data = data.replace(to_replace='TV-Y', value=0)\n",
    "    data = data.replace(to_replace='TV-Y7', value=7)\n",
    "    data = data.replace(to_replace='TV-14', value=14)\n",
    "    \n",
    "    data = data.replace(to_replace='Not Rated', value=0)\n",
    "    data = data.replace(to_replace='Unrated', value=0)\n",
    "    data = data.replace(to_replace='Approved', value=0)\n",
    "    data = data.replace(to_replace='Passed', value=0)\n",
    "    \n",
    "    data = data.replace(to_replace='X', value=17)\n",
    "    data = data.replace(to_replace='M', value=17)\n",
    "    data = data.replace(to_replace='GP', value=12)\n",
    "    \n",
    "    return data\n",
    "\n",
    "movie_data = content_to_numerical(movie_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing NaNs with averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6648.62979592\n",
      "1684.1759902\n",
      "653.779595175\n",
      "2.1284344634\n",
      "39892924.8304\n",
      "9849.74408163\n",
      "692.662510221\n",
      "48514997.6517\n",
      "2002.39901881\n",
      "108.243568804\n",
      "1.36272504092\n",
      "13.6882617062\n"
     ]
    }
   ],
   "source": [
    "def replace_NaNs(col):\n",
    "    # compute average\n",
    "    avg = np.sum(col) / (len(col) - np.sum(col.isnull()))\n",
    "    print(avg)\n",
    "    \n",
    "    # replace NaNs with average\n",
    "    col = col.fillna(value=avg)\n",
    "    return col\n",
    "\n",
    "pd.options.mode.chained_assignment= None\n",
    "\n",
    "to_replace_NaNs = ['actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes', \n",
    "               'aspect_ratio', 'budget', 'cast_total_facebook_likes', 'director_facebook_likes','gross', 'title_year'\n",
    "                   , 'duration', 'facenumber_in_poster', 'content_rating']\n",
    "for column in to_replace_NaNs:\n",
    "    movie_data[column] = replace_NaNs(movie_data[column])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Delete columns with too little 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci-Fi :\n",
      "aantal samples: 596\n",
      "History :\n",
      "aantal samples: 203\n",
      "News :\n",
      "aantal samples: 3\n",
      "Crime :\n",
      "aantal samples: 858\n",
      "Family :\n",
      "aantal samples: 532\n",
      "Short :\n",
      "aantal samples: 3\n",
      "Reality-TV :\n",
      "aantal samples: 0\n",
      "Fantasy :\n",
      "aantal samples: 593\n",
      "Romance :\n",
      "aantal samples: 1088\n",
      "Animation :\n",
      "aantal samples: 233\n",
      "Documentary :\n",
      "aantal samples: 111\n",
      "Film-Noir :\n",
      "aantal samples: 6\n",
      "Adventure :\n",
      "aantal samples: 910\n",
      "Musical :\n",
      "aantal samples: 132\n",
      "Sport :\n",
      "aantal samples: 181\n",
      "Western :\n",
      "aantal samples: 97\n",
      "Drama :\n",
      "aantal samples: 2509\n",
      "Mystery :\n",
      "aantal samples: 475\n",
      "Thriller :\n",
      "aantal samples: 1375\n",
      "Biography :\n",
      "aantal samples: 292\n",
      "Horror :\n",
      "aantal samples: 550\n",
      "Comedy :\n",
      "aantal samples: 1824\n",
      "Action :\n",
      "aantal samples: 1131\n",
      "War :\n",
      "aantal samples: 208\n",
      "Game-Show :\n",
      "aantal samples: 0\n",
      "Music :\n",
      "aantal samples: 213\n"
     ]
    }
   ],
   "source": [
    "for genre in genres_set:\n",
    "    print(genre, ':')\n",
    "    counter = 0\n",
    "    for value in movie_data[genre]:\n",
    "        if value == 1.0:\n",
    "            counter+=1\n",
    "    print('aantal samples:' ,counter)\n",
    "    \n",
    "weghalen = ['Game-Show', 'News', 'Reality-TV', 'Short', 'Film-Noir']\n",
    "\n",
    "for name in weghalen:\n",
    "    del movie_data[name]\n",
    "    \n",
    "#print(movie_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainset en testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aantal training samples:  2940\n",
      "aantal validation samples:  980\n",
      "aantal test samples:  980\n"
     ]
    }
   ],
   "source": [
    "ratings = movie_data['imdb_score'].values\n",
    "del movie_data['imdb_score']\n",
    "\n",
    "X = movie_data.values\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "number_of_samples = len(ratings)\n",
    "np.random.seed(0)\n",
    "random_indices = np.random.permutation(number_of_samples)\n",
    "num_training_samples = round(number_of_samples*0.6)\n",
    "num_validation_samples = round(number_of_samples*0.2) + num_training_samples\n",
    "\n",
    "movie_training = X_std[random_indices[:num_training_samples]]\n",
    "ratings_training = ratings[random_indices[:num_training_samples]]\n",
    "training_indices = random_indices[:num_training_samples]\n",
    "\n",
    "movie_validation = X_std[random_indices[num_training_samples:num_validation_samples]]\n",
    "ratings_validation = ratings[random_indices[num_training_samples:num_validation_samples]]\n",
    "validation_indices = random_indices[num_training_samples:num_validation_samples]\n",
    "\n",
    "movie_test = X_std[random_indices[num_validation_samples:]]\n",
    "ratings_test = ratings[random_indices[num_validation_samples:]]\n",
    "test_indices = random_indices[num_validation_samples:]\n",
    "\n",
    "ratings_training = list(ratings_training)\n",
    "\n",
    "print('aantal training samples: ', len(ratings_training))\n",
    "print('aantal validation samples: ', len(ratings_validation))\n",
    "print('aantal test samples: ', len(ratings_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relative_error(y_predict, y):\n",
    "    \n",
    "    error = 0\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        error += (abs(y_predict[i]-y[i]))/y[i]\n",
    "    training_error = error/len(y)*100\n",
    "    \n",
    "    return training_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 10.411694919423319 percent in neural network algorithm\n",
      "Test error = 10.934604009383447 percent in neural network algorithm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "neural = MLPRegressor(hidden_layer_sizes =(100), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "\n",
    "neural.fit(movie_training, ratings_training)\n",
    "y_neural_train = neural.predict(movie_training)\n",
    "y_neural_test = neural.predict(movie_test)\n",
    "\n",
    "training_error = relative_error(y_neural_train, ratings_training)\n",
    "\n",
    "print(\"Train error = \"+'{}'.format(training_error) + \" percent\"+\" in neural network algorithm\")\n",
    "\n",
    "test_error = relative_error(y_neural_test,ratings_test)\n",
    "\n",
    "print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in neural network algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 10.411694919423319 percent in knn algorithm\n",
      "Test error = 12.253240376043191 percent in knn algorithm\n"
     ]
    }
   ],
   "source": [
    "neighbors = KNeighborsRegressor(n_neighbors = 15)\n",
    "neighbors.fit(movie_training, ratings_training)\n",
    "y_neighbors_train = neighbors.predict(movie_training)\n",
    "\n",
    "train_error = relative_error(y_neighbors_train,ratings_training)\n",
    "\n",
    "print(\"Train error = \"+'{}'.format(training_error)+\" percent\"+\" in knn algorithm\")\n",
    "\n",
    "y_neighbors_test = neighbors.predict(movie_test)\n",
    "\n",
    "test_error = relative_error(y_neighbors_test, ratings_test)\n",
    "\n",
    "print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in knn algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 9.150128853631939\n",
      "Test error = 10.858612788391122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR, NuSVR, LinearSVR\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(movie_training, ratings_training)\n",
    "y_svr_train = svr.predict(movie_training)\n",
    "\n",
    "train_error = relative_error(y_svr_train, ratings_training)\n",
    "\n",
    "print (\"Train error = \"+'{}'.format(train_error))\n",
    "\n",
    "y_svr_test = svr.predict(movie_test)\n",
    "\n",
    "test_error = relative_error(y_svr_test, ratings_test)\n",
    "\n",
    "print (\"Test error = \"+'{}'.format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 10.411694919423319 percent in decision trees\n",
      "Test error = 14.311636958798108 percent in decision trees\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "trees = DecisionTreeRegressor()\n",
    "trees.fit(movie_training, ratings_training)\n",
    "y_trees_train = trees.predict(movie_training)\n",
    "\n",
    "test_error = relative_error(y_trees_train, ratings_training)\n",
    "\n",
    "print(\"Train error = \"+'{}'.format(training_error)+\" percent\"+\" in decision trees\")\n",
    "\n",
    "y_trees_test = trees.predict(movie_test)\n",
    "\n",
    "test_error = relative_error(y_trees_test, ratings_test)\n",
    "\n",
    "print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in decision trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "\n",
    "def random_forest(movie_training, ratings_training, movie_test, ratings_test):\n",
    "    \n",
    "    random_forest = RandomForestRegressor(n_estimators = 20)\n",
    "    random_forest.fit(movie_training, ratings_training)\n",
    "    y_forest_train = random_forest.predict(movie_training)\n",
    "\n",
    "    test_error = relative_error(y_forest_train, ratings_training)\n",
    "\n",
    "    #print(\"Train error = \"+'{}'.format(training_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    y_forest_test = random_forest.predict(movie_test)\n",
    "\n",
    "    test_error = relative_error(y_forest_test, ratings_test)\n",
    "\n",
    "    #print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in random forest\")\n",
    "    \n",
    "    return y_forest_test\n",
    "\n",
    "y_forest_test = random_forest(movie_training, ratings_training, movie_test, ratings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely randomized trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extra_trees(movie_training, ratings_training, movie_test, ratings_test):\n",
    "\n",
    "    extra = ExtraTreesRegressor(n_estimators = 20)\n",
    "    extra.fit(movie_training, ratings_training)\n",
    "    y_extra_train = extra.predict(movie_training)\n",
    "\n",
    "    test_error = relative_error(y_extra_train, ratings_training)\n",
    "\n",
    "    #print(\"Train error = \"+'{}'.format(training_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    y_extra_test = extra.predict(movie_test)\n",
    "\n",
    "    test_error = relative_error(y_extra_test, ratings_test)\n",
    "\n",
    "    #print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in random forest\")\n",
    "    \n",
    "    return y_extra_test\n",
    "\n",
    "y_extra_test = extra_trees(movie_training, ratings_training, movie_test, ratings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 10.411694919423319 percent in random forest\n",
      "Test error = 10.425263913340322 percent in random forest\n"
     ]
    }
   ],
   "source": [
    "#regressor  = BaggingRegressor(MLPRegressor(max_iter = 300))\n",
    "bagging = BaggingRegressor(RandomForestRegressor())\n",
    "bagging.fit(movie_training, ratings_training)\n",
    "y_bagging_train = bagging.predict(movie_training)\n",
    "\n",
    "test_error = relative_error(y_bagging_train ,ratings_training)\n",
    "\n",
    "print(\"Train error = \"+'{}'.format(training_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "y_bagging_test = bagging.predict(movie_test)\n",
    "\n",
    "test_error = relative_error(y_bagging_test, ratings_test)\n",
    "\n",
    "print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in random forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neural_1 = MLPRegressor(hidden_layer_sizes =(100), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "# neural_2 = MLPRegressor(hidden_layer_sizes =(50), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "# neural_3 = MLPRegressor(hidden_layer_sizes =(10), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "# neural_4 = MLPRegressor(hidden_layer_sizes =(200), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "\n",
    "# neural_1.fit(movie_training, ratings_training)\n",
    "# neural_2.fit(movie_training, ratings_training)\n",
    "# neural_3.fit(movie_training, ratings_training)\n",
    "# neural_4.fit(movie_training, ratings_training)\n",
    "\n",
    "# y_1_neural = neural_1.predict(movie_validation)\n",
    "# y_2_neural = neural_2.predict(movie_validation)\n",
    "# y_3_neural = neural_3.predict(movie_validation)\n",
    "# y_4_neural = neural_4.predict(movie_validation)\n",
    "\n",
    "# error_1_neural = relative_error(y_1_neural, ratings_validation)\n",
    "# error_2_neural = relative_error(y_2_neural, ratings_validation)\n",
    "# error_3_neural = relative_error(y_3_neural, ratings_validation)\n",
    "# error_4_neural = relative_error(y_4_neural, ratings_validation)\n",
    "\n",
    "# print(error_1_neural)\n",
    "# print(error_2_neural)\n",
    "# print(error_3_neural)\n",
    "# print(error_4_neural)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# de parameters die kunnen worden aangepast \n",
    "\n",
    "# svr_1 = SVR(C = 0.0001)\n",
    "# svr_2 = SVR(C = 0.01)\n",
    "# svr_3 = SVR(C = 1.0)\n",
    "# svr_4 = SVR(C = 100)\n",
    "# svr_5 = SVR(C = 10000)\n",
    "\n",
    "# svr_1 = SVR(epsilon = 0.001)\n",
    "# svr_2 = SVR(epsilon = 0.01)\n",
    "# svr_3 = SVR(epsilon = 0.1)\n",
    "# svr_4 = SVR(epsilon = 10)\n",
    "# svr_5 = SVR(epsilon = 100)\n",
    "\n",
    "# svr_1 = SVR(kernel = 'rbf')\n",
    "# svr_2 = SVR(kernel = 'linear')\n",
    "# svr_3 = SVR(kernel = 'poly')\n",
    "# svr_4 = SVR()\n",
    "# svr_5 = SVR()\n",
    "\n",
    "# svr_1.fit(movie_training, ratings_training)\n",
    "# svr_2.fit(movie_training, ratings_training)\n",
    "# svr_3.fit(movie_training, ratings_training)\n",
    "# svr_4.fit(movie_training, ratings_training)\n",
    "# svr_5.fit(movie_training, ratings_training)\n",
    "\n",
    "# y_1_svr = svr_1.predict(movie_validation)\n",
    "# y_2_svr = svr_2.predict(movie_validation)\n",
    "# y_3_svr = svr_3.predict(movie_validation)\n",
    "# y_4_svr = svr_4.predict(movie_validation)\n",
    "# y_5_svr = svr_5.predict(movie_validation)\n",
    "\n",
    "# error_1_svr = relative_error(y_1_svr, ratings_validation)\n",
    "# error_2_svr = relative_error(y_2_svr, ratings_validation)\n",
    "# error_3_svr = relative_error(y_3_svr, ratings_validation)\n",
    "# error_4_svr = relative_error(y_4_svr, ratings_validation)\n",
    "# error_5_svr = relative_error(y_5_svr, ratings_validation)\n",
    "\n",
    "# print(error_1_svr)\n",
    "# print(error_2_svr)\n",
    "# print(error_3_svr)\n",
    "# print(error_4_svr)\n",
    "# print(error_5_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# forest_1 = RandomForestRegressor(n_estimators = 5)\n",
    "# forest_2 = RandomForestRegressor(n_estimators = 10)\n",
    "# forest_3 = RandomForestRegressor(n_estimators = 20)\n",
    "# forest_4 = RandomForestRegressor(n_estimators = 40)\n",
    "# forest_5 = RandomForestRegressor(n_estimators = 80)\n",
    "\n",
    "# forest_1.fit(movie_training, ratings_training)\n",
    "# forest_2.fit(movie_training, ratings_training)\n",
    "# forest_3.fit(movie_training, ratings_training)\n",
    "# forest_4.fit(movie_training, ratings_training)\n",
    "# forest_5.fit(movie_training, ratings_training)\n",
    "\n",
    "# y_1_forest = forest_1.predict(movie_validation)\n",
    "# y_2_forest = forest_2.predict(movie_validation)\n",
    "# y_3_forest = forest_3.predict(movie_validation)\n",
    "# y_4_forest = forest_4.predict(movie_validation)\n",
    "# y_5_forest = forest_5.predict(movie_validation)\n",
    "\n",
    "# error_1_forest = relative_error(y_1_forest, ratings_validation)\n",
    "# error_2_forest = relative_error(y_2_forest, ratings_validation)\n",
    "# error_3_forest = relative_error(y_3_forest, ratings_validation)\n",
    "# error_4_forest = relative_error(y_4_forest, ratings_validation)\n",
    "# error_5_forest = relative_error(y_5_forest, ratings_validation)\n",
    "\n",
    "# print(error_1_forest)\n",
    "# print(error_2_forest)\n",
    "# print(error_3_forest)\n",
    "# print(error_4_forest)\n",
    "# print(error_5_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely randomized trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extra_1 = ExtraTreesRegressor(n_estimators = 5)\n",
    "# extra_2 = ExtraTreesRegressor(n_estimators = 10)\n",
    "# extra_3 = ExtraTreesRegressor(n_estimators = 20)\n",
    "# extra_4 = ExtraTreesRegressor(n_estimators = 40)\n",
    "# extra_5 = ExtraTreesRegressor(n_estimators = 80)\n",
    "\n",
    "# # extra_1 = ExtraTreesRegressor(n_estimators = 20, max_features = 'auto')\n",
    "# # extra_2 = ExtraTreesRegressor(n_estimators = 20, max_features = 'sqrt')\n",
    "# # extra_3 = ExtraTreesRegressor(n_estimators = 20, max_features = 'log2')\n",
    "# # extra_4 = ExtraTreesRegressor()\n",
    "# # extra_5 = ExtraTreesRegressor()\n",
    "\n",
    "# extra_1.fit(movie_training, ratings_training)\n",
    "# extra_2.fit(movie_training, ratings_training)\n",
    "# extra_3.fit(movie_training, ratings_training)\n",
    "# extra_4.fit(movie_training, ratings_training)\n",
    "# extra_5.fit(movie_training, ratings_training)\n",
    "\n",
    "# y_1_extra = extra_1.predict(movie_validation)\n",
    "# y_2_extra = extra_2.predict(movie_validation)\n",
    "# y_3_extra = extra_3.predict(movie_validation)\n",
    "# y_4_extra = extra_4.predict(movie_validation)\n",
    "# y_5_extra = extra_5.predict(movie_validation)\n",
    "\n",
    "# error_1_extra = relative_error(y_1_extra, ratings_validation)\n",
    "# error_2_extra = relative_error(y_2_extra, ratings_validation)\n",
    "# error_3_extra = relative_error(y_3_extra, ratings_validation)\n",
    "# error_4_extra = relative_error(y_4_extra, ratings_validation)\n",
    "# error_5_extra = relative_error(y_5_extra, ratings_validation)\n",
    "\n",
    "# print(error_1_extra)\n",
    "# print(error_2_extra)\n",
    "# print(error_3_extra)\n",
    "# print(error_4_extra)\n",
    "# print(error_5_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neighbor_1 = KNeighborsRegressor(n_neighbors = 1)\n",
    "# neighbor_2 = KNeighborsRegressor(n_neighbors = 5)\n",
    "# neighbor_3 = KNeighborsRegressor(n_neighbors = 10)\n",
    "# neighbor_4 = KNeighborsRegressor(n_neighbors = 20)\n",
    "# neighbor_5 = KNeighborsRegressor(n_neighbors = 40)\n",
    "\n",
    "# neighbor_1.fit(movie_training, ratings_training)\n",
    "# neighbor_2.fit(movie_training, ratings_training)\n",
    "# neighbor_3.fit(movie_training, ratings_training)\n",
    "# neighbor_4.fit(movie_training, ratings_training)\n",
    "# neighbor_5.fit(movie_training, ratings_training)\n",
    "\n",
    "# y_1_neighbor = neighbor_1.predict(movie_validation)\n",
    "# y_2_neighbor = neighbor_2.predict(movie_validation)\n",
    "# y_3_neighbor = neighbor_3.predict(movie_validation)\n",
    "# y_4_neighbor = neighbor_4.predict(movie_validation)\n",
    "# y_5_neighbor = neighbor_5.predict(movie_validation)\n",
    "\n",
    "# error_1_neighbor = relative_error(y_1_neighbor, ratings_validation)\n",
    "# error_2_neighbor = relative_error(y_2_neighbor, ratings_validation)\n",
    "# error_3_neighbor = relative_error(y_3_neighbor, ratings_validation)\n",
    "# error_4_neighbor = relative_error(y_4_neighbor, ratings_validation)\n",
    "# error_5_neighbor = relative_error(y_5_neighbor, ratings_validation)\n",
    "\n",
    "# print(error_1_neighbor)\n",
    "# print(error_2_neighbor)\n",
    "# print(error_3_neighbor)\n",
    "# print(error_4_neighbor)\n",
    "# print(error_5_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks and svr and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_ensembles = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    y_forest_ens = random_forest(movie_training, ratings_training, movie_test, ratings_test)\n",
    "    y_extra_ens = extra_trees(movie_training, ratings_training, movie_test, ratings_test)\n",
    "    prediction = [sum(x)/3 for x in zip(y_svr_test, y_forest_ens, y_extra_ens)]\n",
    "    error_ensemble = relative_error(prediction, ratings_test)\n",
    "    \n",
    "    error_ensembles.append(error_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "error_ensembles = np.array(error_ensembles)\n",
    "    \n",
    "mu = np.sum(error_ensembles)/len(error_ensembles)\n",
    "sigma = (np.square(np.sum(error_ensembles-mu)))/len(error_ensembles)\n",
    "\n",
    "print(mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8b6ee5d45864>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0muitschieters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mratings_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "uitschieters = []\n",
    "\n",
    "for i in range(len(prediction)):\n",
    "    if abs(prediction[i]-ratings_test[i])>1.0:\n",
    "        counter+= 1\n",
    "        index = i\n",
    "        uitschieters.append(i)\n",
    "        #print(prediction[i], ratings_test[i])    \n",
    "\n",
    "print('fractie die meer dan twee punten verschilt = ', counter/980)\n",
    "indices = test_indices[uitschieters]\n",
    "#print(data.iloc[indices]['budget'].describe())\n",
    "print('fractie die NaN heeft bij gross: ', data.iloc[indices]['gross'].isnull().sum()/len(uitschieters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
