{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'director_name', 'num_critic_for_reviews', 'duration', 'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name', 'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name', 'movie_title', 'num_voted_users', 'cast_total_facebook_likes', 'actor_3_name', 'facenumber_in_poster', 'plot_keywords', 'movie_imdb_link', 'num_user_for_reviews', 'language', 'country', 'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes', 'imdb_score', 'aspect_ratio', 'movie_facebook_likes']\n",
      "2698\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "NUM_THRESHOLDS = 6\n",
    "\n",
    "#reads in the data\n",
    "data = pd.read_csv('movie_metadata.csv')\n",
    "print(list(data))\n",
    "print(data.isnull().sum().sum())\n",
    "#removes the values that aren't numeric\n",
    "to_drop = ['director_name', 'num_critic_for_reviews', 'actor_2_name','actor_1_name', 'movie_title', 'num_voted_users'\n",
    "           , 'actor_3_name', 'plot_keywords', 'movie_imdb_link', 'num_user_for_reviews']\n",
    "\n",
    "#makes the new data set without the to_drop colums\n",
    "features_list = data.columns.difference(to_drop)\n",
    "movie_data = data[features_list]\n",
    "#print(np.sum(movie_num.isnull()))\n",
    "#print(movie_data.content_rating.unique())\n",
    "pd.options.mode.chained_assignment = None \n",
    "   \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Genres into individual Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['Sci-Fi', 'Crime', 'Romance', 'Animation', 'Music', 'Comedy', 'War', 'Horror', 'Film-Noir', 'Adventure', 'News', 'Reality-TV', 'Thriller', 'Western', 'Mystery', 'Short', 'Drama', 'Action', 'Documentary', 'Musical', 'History', 'Family', 'Fantasy', 'Game-Show', 'Sport', 'Biography'])\n"
     ]
    }
   ],
   "source": [
    "# make a set with all unique genres\n",
    "\n",
    "genres = []\n",
    "\n",
    "for string in movie_data['genres']:\n",
    "    genre = string.split('|')\n",
    "    genres = genres+genre\n",
    "    \n",
    "genres_set = set(genres)\n",
    "print(genres_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "genres_dict = dict()\n",
    "for genre in genres_set:\n",
    "    genres_dict[genre] = []\n",
    "    \n",
    "for string in movie_data.genres:\n",
    "    genres = string.split('|')\n",
    "    for genre in genres_set:\n",
    "        if genre in genres:\n",
    "            genres_dict[genre] = genres_dict[genre]+[1]\n",
    "        else:\n",
    "            genres_dict[genre] = genres_dict[genre]+[0]\n",
    "\n",
    "del movie_data['genres']\n",
    "#print(genres_dict['Short'])\n",
    "\n",
    "for genre in genres_set:\n",
    "    series = pd.Series(genres_dict[genre])\n",
    "    movie_data[genre] = series\n",
    "\n",
    "#print(movie_data['Short'].values)    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before deletion: 5043\n",
      "Length after deletion: 3771\n",
      "Length before deletion: 5043\n",
      "Length after deletion: 4490\n",
      "Length before deletion: 5043\n",
      "Length after deletion: 4777\n",
      "Length before deletion: 5043\n",
      "Length after deletion: 4900\n",
      "Length before deletion: 5043\n",
      "Length after deletion: 4983\n",
      "Length before deletion: 5043\n",
      "Length after deletion: 5023\n"
     ]
    }
   ],
   "source": [
    "NA_THRESH = 4\n",
    "\n",
    "#deletes the row when there are more or equal to the threshod number of NaN's\n",
    "def remove_too_many_NaN(data, threshold):\n",
    "    print(\"Length before deletion: {}\".format(len(data)))\n",
    "\n",
    "    remove_indices = []\n",
    "    for index, nNaN in data.isnull().sum(axis=1).iteritems():\n",
    "        if nNaN >= threshold:\n",
    "            remove_indices.append(index)\n",
    "    \n",
    "    tempcopy = data.copy()\n",
    "    \n",
    "    # drop movies with too many NaNs\n",
    "    data = data.drop(data.index[remove_indices])\n",
    "    removed_films = tempcopy.drop(tempcopy.index[remove_indices])\n",
    "    \n",
    "    print(\"Length after deletion: {}\".format(len(data)))\n",
    "    \n",
    "    return data\n",
    "\n",
    "tempcopy = movie_data.copy()\n",
    "movie_data = []\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    tempremoved = remove_too_many_NaN(tempcopy.copy(), i + 1)\n",
    "    movie_data.append(tempremoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting number of NaNs per threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_num_NaNs(movie_data, marked):\n",
    "    nans = []\n",
    "    thresh = []\n",
    "    samps = []\n",
    "    for i in range(NUM_THRESHOLDS):\n",
    "        numnans = movie_data[i].isnull().sum().sum()\n",
    "        nans.append(numnans)\n",
    "        samps.append(len(movie_data[i]))\n",
    "        thresh.append(i + 1)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "#     ax = fig.add_subplot(121)\n",
    "#     plt.plot(thresh, nans, 'bx-')\n",
    "#     plt.plot(thresh[3], nans[3], 'ro')\n",
    "#     for xy in zip(thresh, nans):\n",
    "#         ax.annotate('%s' % xy[0], xy=xy, textcoords='data')\n",
    "#     plt.ylabel('Number of NaNs')\n",
    "#     plt.xlabel('Threshold')\n",
    "    \n",
    "    ax2 = fig.add_subplot(111)\n",
    "    plt.plot(thresh, samps, 'rx-')\n",
    "    plt.plot(thresh[marked], samps[marked], 'bo')\n",
    "    for xy in zip(thresh, samps):\n",
    "        ax2.annotate('%s' % xy[0], xy=xy, textcoords='data')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_num_NaNs(movie_data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Color to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def numeric_color(movie_data):\n",
    "    #replaces the color value to 1\n",
    "    movie_data.color = movie_data.color.replace(to_replace = 'Color', value = 1)\n",
    "    movie_data.color = movie_data.color.replace(to_replace = 'NaN', value = 1)\n",
    "\n",
    "    #replaces the NaN or black and white values to 0\n",
    "    for item in movie_data.color:\n",
    "        if item != 1:\n",
    "            movie_data.color = movie_data.color.replace(to_replace = item, value = 0)\n",
    "\n",
    "    #movie_data.color = new_color_column \n",
    "    #print(movie_data['color'])\n",
    "\n",
    "    #makes sure that there is nog error where it shouldn't be\n",
    "    pd.options.mode.chained_assignment= None\n",
    "    print('Done')\n",
    "    return movie_data\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    movie_data[i] = numeric_color(movie_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Country to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def numeric_country(movie_data):\n",
    "    #replaces the USA values to 1\n",
    "    movie_data.country = movie_data.country.replace(to_replace ='USA', value = 1)\n",
    "    movie_data.country = movie_data.country.replace(to_replace ='NaN', value = 1)\n",
    "    #replaces the NaN or non USA values to 0\n",
    "    for item in movie_data.country:\n",
    "        if item != 1:\n",
    "            movie_data.country = movie_data.country.replace(to_replace = item, value = 0)\n",
    "\n",
    "    #print(movie_data['country'])\n",
    "\n",
    "    #makes sure that there is nog error where it shouldn't be\n",
    "    pd.options.mode.chained_assignment= None\n",
    "\n",
    "    print('Done')\n",
    "    return movie_data\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    movie_data[i] = numeric_country(movie_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Languange to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def numeric_language(movie_data):\n",
    "    #replaces the English values to 1\n",
    "    movie_data.language = movie_data.language.replace(to_replace = 'English', value = 1)\n",
    "    movie_data.language = movie_data.language.replace(to_replace = 'NaN', value = 1)\n",
    "\n",
    "    #replaces the other values to 0\n",
    "    for item in movie_data.language:\n",
    "        if item != 1:\n",
    "            movie_data.language = movie_data.language.replace(to_replace = item, value = 0)\n",
    "\n",
    "    #print(movie_data['language'])\n",
    "\n",
    "    #makes sure that there is nog error where it shouldn't be\n",
    "    pd.options.mode.chained_assignment= None\n",
    "    print('Done')\n",
    "    return movie_data\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    movie_data[i] = numeric_language(movie_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaning Content_Rating to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def content_to_numerical(data):\n",
    "    data = data.replace(to_replace='G', value=0)\n",
    "    data = data.replace(to_replace='PG', value=12)\n",
    "    data = data.replace(to_replace='PG-13', value=13)\n",
    "    data = data.replace(to_replace='R', value=17)\n",
    "    data = data.replace(to_replace='NC-17', value=17)\n",
    "    \n",
    "    data = data.replace(to_replace='TV-PG', value=12)\n",
    "    data = data.replace(to_replace='TV-MA', value=17)\n",
    "    data = data.replace(to_replace='TV-G', value=0)\n",
    "    data = data.replace(to_replace='TV-Y', value=0)\n",
    "    data = data.replace(to_replace='TV-Y7', value=7)\n",
    "    data = data.replace(to_replace='TV-14', value=14)\n",
    "    \n",
    "    data = data.replace(to_replace='Not Rated', value=0)\n",
    "    data = data.replace(to_replace='Unrated', value=0)\n",
    "    data = data.replace(to_replace='Approved', value=0)\n",
    "    data = data.replace(to_replace='Passed', value=0)\n",
    "    \n",
    "    data = data.replace(to_replace='X', value=17)\n",
    "    data = data.replace(to_replace='M', value=17)\n",
    "    data = data.replace(to_replace='GP', value=12)\n",
    "    \n",
    "    return data\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    movie_data[i] = content_to_numerical(movie_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing NaNs with averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7722.74303898\n",
      "2014.86157518\n",
      "768.95332803\n",
      "2.11111376293\n",
      "46074665.6696\n",
      "11486\n",
      "804.246353752\n",
      "52410371.7385\n",
      "2003.01060727\n",
      "110.219570406\n",
      "1.37708830549\n",
      "14\n",
      "7032.59888641\n",
      "1792.34922049\n",
      "691.49108337\n",
      "2.1096122633\n",
      "41840819.2828\n",
      "10423\n",
      "744.864587973\n",
      "49287524.5918\n",
      "2001.97906459\n",
      "109.24142539\n",
      "1.35540691193\n",
      "13.7810398924\n",
      "6747.99685995\n",
      "1713.49706868\n",
      "664.698322851\n",
      "2.11774347826\n",
      "40444270.7169\n",
      "10003\n",
      "703.776847394\n",
      "48619441.8755\n",
      "2002.19866025\n",
      "108.555276382\n",
      "1.36479966436\n",
      "13.6992675571\n",
      "6648.62979592\n",
      "1684.1759902\n",
      "653.779595175\n",
      "2.1284344634\n",
      "39892924.8304\n",
      "9849\n",
      "692.662510221\n",
      "48514997.6517\n",
      "2002.39901881\n",
      "108.243568804\n",
      "1.36272504092\n",
      "13.6882617062\n",
      "6558.490263\n",
      "1664.34712736\n",
      "648.761167002\n",
      "2.19000853607\n",
      "39813189.6557\n",
      "9728\n",
      "688.57729027\n",
      "48491666.3264\n",
      "2002.45062983\n",
      "107.495680129\n",
      "1.37183098592\n",
      "13.6780054933\n",
      "6577.96372334\n",
      "1656.08695652\n",
      "646.135037954\n",
      "2.2200721562\n",
      "39769410.9927\n",
      "9727\n",
      "686.888168558\n",
      "48468407.5268\n",
      "2002.46838265\n",
      "107.390618762\n",
      "1.37005988024\n",
      "13.6774261603\n"
     ]
    }
   ],
   "source": [
    "def replace_NaNs(col):\n",
    "    # compute average\n",
    "    avg = np.sum(col) / (len(col) - np.sum(col.isnull()))\n",
    "    print(avg)\n",
    "    \n",
    "    # replace NaNs with average\n",
    "    col = col.fillna(value=avg)\n",
    "    return col\n",
    "\n",
    "pd.options.mode.chained_assignment= None\n",
    "\n",
    "to_replace_NaNs = ['actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes', \n",
    "               'aspect_ratio', 'budget', 'cast_total_facebook_likes', 'director_facebook_likes','gross', 'title_year'\n",
    "                   , 'duration', 'facenumber_in_poster', 'content_rating']\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    for column in to_replace_NaNs:\n",
    "        movie_data[i][column] = replace_NaNs(movie_data[i][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Delete columns with too little 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def del_sparse_cols(movie_data, doprint=False):\n",
    "    for genre in genres_set:\n",
    "        if doprint:\n",
    "            print(genre, ':')\n",
    "        counter = 0\n",
    "        for value in movie_data[genre]:\n",
    "            if value == 1.0:\n",
    "                counter+=1\n",
    "        \n",
    "        if doprint:\n",
    "            print(counter)\n",
    "\n",
    "    weghalen = ['Game-Show', 'News', 'Reality-TV', 'Short', 'Film-Noir']\n",
    "\n",
    "    for name in weghalen:\n",
    "        del movie_data[name]\n",
    "\n",
    "    #print(movie_data)\n",
    "    return movie_data\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    movie_data[i] = del_sparse_cols(movie_data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cor(data):\n",
    "    plt.figure(figsize=(10,10), tight_layout=True)\n",
    "    plt.pcolor(data.corr())\n",
    "    plt.yticks(np.arange(0.5, len(list(data)), 1), list(data))\n",
    "    plt.xticks(np.arange(0.5, len(list(data)), 1), list(data), rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "#plot_cor(movie_data_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainset en testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263\n",
      "754\n",
      "754\n",
      "2694\n",
      "898\n",
      "898\n",
      "2866\n",
      "955\n",
      "956\n",
      "2940\n",
      "980\n",
      "980\n",
      "2990\n",
      "997\n",
      "996\n",
      "3014\n",
      "1005\n",
      "1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/carlo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/carlo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/carlo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:22: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/carlo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/carlo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:25: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "def split_to_sets(movie_data):\n",
    "    movie_data_sets = {'movie_training': 0, 'ratings_training': 0, \n",
    "                       'movie_validation': 0, 'ratings_validation': 0, \n",
    "                       'movie_test': 0, 'ratings_test': 0}\n",
    "    \n",
    "    ratings = movie_data['imdb_score'].values\n",
    "    del movie_data['imdb_score']\n",
    "\n",
    "    X = movie_data.values\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "    number_of_samples = len(ratings)\n",
    "    np.random.seed(0)\n",
    "    random_indices = np.random.permutation(number_of_samples)\n",
    "    num_training_samples = round(number_of_samples*0.6)\n",
    "    num_validation_samples = round(number_of_samples*0.2)+num_training_samples\n",
    "\n",
    "    movie_data_sets['movie_training'] = X_std[random_indices[:num_training_samples]]\n",
    "    ratings_training = ratings[random_indices[:num_training_samples]]\n",
    "\n",
    "    movie_data_sets['movie_validation'] = X_std[random_indices[num_training_samples:num_validation_samples]]\n",
    "    movie_data_sets['ratings_validation'] = ratings[random_indices[num_training_samples:num_validation_samples]]\n",
    "\n",
    "    movie_data_sets['movie_test'] = X_std[random_indices[num_validation_samples:]]\n",
    "    movie_data_sets['ratings_test'] = ratings[random_indices[num_validation_samples:]]\n",
    "\n",
    "    movie_data_sets['ratings_training'] = list(ratings_training)\n",
    "\n",
    "    print(len(movie_data_sets['ratings_training']))\n",
    "    print(len(movie_data_sets['ratings_validation']))\n",
    "    print(len(movie_data_sets['ratings_test']))\n",
    "    \n",
    "    return movie_data_sets\n",
    "\n",
    "sets = []\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    sets.append(split_to_sets(movie_data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relative_error(y_predict, y):\n",
    "    \n",
    "    error = 0\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        error += (abs(y_predict[i]-y[i]))/y[i]\n",
    "    training_error = error/len(y)*100\n",
    "    \n",
    "    return training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyse_predictions(regr, sets, statdict, doprint=False):\n",
    "    x_test = sets['movie_test']\n",
    "    y_test = sets['ratings_test']\n",
    "    \n",
    "    regr.fit (sets['movie_training'], sets['ratings_training'])\n",
    "\n",
    "    prediction = regr.predict(x_test)\n",
    "    results_list.append(prediction)\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % np.mean((prediction - y_test) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % regr.score(x_test, y_test))\n",
    "\n",
    "    \n",
    "    \n",
    "    predres = list(regr.predict(x_test))\n",
    "    actual = list(y_test)\n",
    "    highest = avg = error = 0\n",
    "    lowest = predres[0]\n",
    "    for i in range(len(predres)):\n",
    "        diff = abs(predres[i] - actual[i])\n",
    "        avg += diff\n",
    "        error += abs(predres[i] - actual[i]) / actual[i]\n",
    "        \n",
    "        if doprint:\n",
    "            print(\"predicted:\", predres[i]) \n",
    "            print (\"\\tActual:\", actual[i])\n",
    "            print (\"\\tDifference:\", abs(predres[i] - actual[i]))\n",
    "\n",
    "        if diff > highest:\n",
    "            highest = diff\n",
    "        if diff < lowest:\n",
    "            lowest = diff\n",
    "\n",
    "    errper = error / len(x_test) * 100\n",
    "    avgdif =  avg / len(predres)\n",
    "    \n",
    "    if doprint:\n",
    "        #print \"Number of Samples:\", len(predres)\n",
    "        #print \"Highest difference:\", highest\n",
    "        #print \"Lowest difference:\", lowest\n",
    "        #print \"average difference:\", avgdif\n",
    "        print(\"Error Percentage:\", errper)\n",
    "    \n",
    "#     statdict['highest'].append(highest)\n",
    "#     statdict['lowest'].append(lowest)\n",
    "#     statdict['avgdif'].append(avgdif)\n",
    "    statdict['errper'].append(errper)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_list = []\n",
    "statdicts = []\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    statdict = {'names':[], 'errper': [], 'predictions': []}\n",
    "    statdicts.append(statdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# def neural(statdict, sets):\n",
    "\n",
    "#     neural = MLPRegressor(hidden_layer_sizes =(100), activation = 'logistic', solver = 'adam', max_iter = 1000)\n",
    "\n",
    "#     neural.fit(sets['movie_training'], sets['ratings_training'])\n",
    "#     y_neural_train = neural.predict(sets['movie_training'])\n",
    "#     y_neural_test = neural.predict(sets['movie_test'])\n",
    "\n",
    "#     training_error = relative_error(y_neural_train, sets['ratings_training'])\n",
    "\n",
    "#     print(\"Train error = \"+'{}'.format(training_error)+\" percent\"+\" in neural network algorithm\")\n",
    "\n",
    "#     test_error = relative_error(y_neural_test,sets['ratings_test'])\n",
    "\n",
    "#     print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in neural network algorithm\")\n",
    "    \n",
    "#     statdict['errper'].append(test_error)\n",
    "#     statdict['names'].append('neural')\n",
    "#     statdict['predictions'].append(y_neural_test)\n",
    "    \n",
    "#     return statdict\n",
    "\n",
    "# for i in range(4):\n",
    "#     statdicts[i] = neural(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 11.0490640796 percent in knn algorithm\n",
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 11.7695538866 percent in knn algorithm\n",
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 13.3180628211 percent in knn algorithm\n",
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 11.8789017784 percent in knn algorithm\n",
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 13.4150840723 percent in knn algorithm\n",
      "Train error = 0.0 percent in knn algorithm\n",
      "Test error = 13.3765837677 percent in knn algorithm\n"
     ]
    }
   ],
   "source": [
    "def do_knn(statdict, sets, n_neighbors=5, weights='uniform'):\n",
    "        \n",
    "    neighbors = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights)\n",
    "    neighbors.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_neighbors_train = neighbors.predict(sets['movie_training'])\n",
    "    #y_predict_train = list(y_predict_train)\n",
    "\n",
    "    train_error = relative_error(y_neighbors_train,sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in knn algorithm\")\n",
    "\n",
    "    y_neighbors_test = neighbors.predict(sets['movie_test'])\n",
    "    #y_predict_test = list(y_predict_test)\n",
    "\n",
    "    test_error = relative_error(y_neighbors_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in knn algorithm\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('kNN')\n",
    "    statdict['predictions'].append(y_neighbors_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    statdicts[i] = do_knn(statdicts[i], sets[i], n_neighbors=15, weights='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 8.18761939314\n",
      "Test error = 10.2504894917\n",
      "Train error = 8.61554189663\n",
      "Test error = 10.9147868316\n",
      "Train error = 8.69726527135\n",
      "Test error = 12.1802303446\n",
      "Train error = 9.15012885363\n",
      "Test error = 10.8586127884\n",
      "Train error = 9.089494534\n",
      "Test error = 12.6239888385\n",
      "Train error = 9.05028697238\n",
      "Test error = 12.6123027094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR, NuSVR, LinearSVR\n",
    "\n",
    "def do_svr(statdict, sets):\n",
    "    svr = SVR()\n",
    "    svr.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_svr_train = svr.predict(sets['movie_training'])\n",
    "    #y_train = list(y_train)\n",
    "\n",
    "    train_error = relative_error(y_svr_train, sets['ratings_training'])\n",
    "\n",
    "    print (\"Train error = \"+'{}'.format(train_error))\n",
    "\n",
    "    y_svr_test = svr.predict(sets['movie_test'])\n",
    "    #y_test = list(y_test)\n",
    "\n",
    "    test_error = relative_error(y_svr_test, sets['ratings_test'])\n",
    "\n",
    "    print (\"Test error = \"+'{}'.format(test_error))\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('SVM')\n",
    "    statdict['predictions'].append(y_svr_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    statdicts[i] = do_svr(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 4.53099272772e-16 percent in decision trees\n",
      "Test error = 12.938106782 percent in decision trees\n",
      "Train error = 3.99570033265e-16 percent in decision trees\n",
      "Test error = 13.0720374377 percent in decision trees\n",
      "Train error = 3.85960584977e-16 percent in decision trees\n",
      "Test error = 15.1725795103 percent in decision trees\n",
      "Train error = 3.4713944456e-16 percent in decision trees\n",
      "Test error = 14.6848133608 percent in decision trees\n",
      "Train error = 3.37434601533e-16 percent in decision trees\n",
      "Test error = 15.9992074466 percent in decision trees\n",
      "Train error = 3.48625318472e-16 percent in decision trees\n",
      "Test error = 15.2397145098 percent in decision trees\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def do_dt(statdict, sets):\n",
    "    trees = DecisionTreeRegressor()\n",
    "    trees.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_trees_train = trees.predict(sets['movie_training'])\n",
    "\n",
    "    train_error = relative_error(y_trees_train, sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in decision trees\")\n",
    "\n",
    "    y_trees_test = trees.predict(sets['movie_test'])\n",
    "\n",
    "    test_error = relative_error(y_trees_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in decision trees\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('DT')\n",
    "    statdict['predictions'].append(y_trees_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    statdicts[i] = do_dt(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 3.73059015199 percent in random forest\n",
      "Test error = 9.81629476237 percent in random forest\n",
      "Train error = 3.85245507305 percent in random forest\n",
      "Test error = 10.4878573693 percent in random forest\n",
      "Train error = 3.88312977613 percent in random forest\n",
      "Test error = 11.4802460286 percent in random forest\n",
      "Train error = 4.01885790857 percent in random forest\n",
      "Test error = 10.5670314125 percent in random forest\n",
      "Train error = 3.94779742525 percent in random forest\n",
      "Test error = 11.9421101078 percent in random forest\n",
      "Train error = 3.97121737625 percent in random forest\n",
      "Test error = 11.7071565053 percent in random forest\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "\n",
    "def do_randforest(statdict, sets):\n",
    "    random_forest = RandomForestRegressor(n_estimators = 20)\n",
    "    random_forest.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_forest_train = random_forest.predict(sets['movie_training'])\n",
    "\n",
    "    train_error = relative_error(y_forest_train, sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    y_forest_test = random_forest.predict(sets['movie_test'])\n",
    "\n",
    "    test_error = relative_error(y_forest_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('Rfor')\n",
    "    statdict['predictions'].append(y_forest_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    statdicts[i] = do_randforest(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely randomized trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error = 1.78592811955e-14 percent in random forest\n",
      "Test error = 9.89456521061 percent in random forest\n",
      "Train error = 1.80124713131e-14 percent in random forest\n",
      "Test error = 10.1359639964 percent in random forest\n",
      "Train error = 1.79403908702e-14 percent in random forest\n",
      "Test error = 11.6106181548 percent in random forest\n",
      "Train error = 1.82594909424e-14 percent in random forest\n",
      "Test error = 10.1771437884 percent in random forest\n",
      "Train error = 1.79587063332e-14 percent in random forest\n",
      "Test error = 11.8318383864 percent in random forest\n",
      "Train error = 1.77806739716e-14 percent in random forest\n",
      "Test error = 11.868334368 percent in random forest\n"
     ]
    }
   ],
   "source": [
    "def do_xrandtrees(statdict, sets):\n",
    "    extra = ExtraTreesRegressor(n_estimators = 20)\n",
    "    extra.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_extra_train = extra.predict(sets['movie_training'])\n",
    "\n",
    "    train_error = relative_error(y_extra_train, sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    y_extra_test = extra.predict(sets['movie_test'])\n",
    "\n",
    "    test_error = relative_error(y_extra_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('XRfor')\n",
    "    statdict['predictions'].append(y_extra_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    statdicts[i] = do_xrandtrees(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_baggingregr(statdict, sets):\n",
    "    #regressor  = BaggingRegressor(MLPRegressor(max_iter = 300))\n",
    "    bagging = BaggingRegressor(RandomForestRegressor())\n",
    "    bagging.fit(sets['movie_training'], sets['ratings_training'])\n",
    "    y_bagging_train = bagging.predict(sets['movie_training'])\n",
    "    #y_predict_train = list(y_predict_train)\n",
    "\n",
    "    train_error = relative_error(y_bagging_train ,sets['ratings_training'])\n",
    "\n",
    "    print(\"Train error = \"+'{}'.format(train_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    y_bagging_test = bagging.predict(sets['movie_test'])\n",
    "    #y_predict_test = list(y_predict_test)\n",
    "\n",
    "    test_error = relative_error(y_bagging_test, sets['ratings_test'])\n",
    "\n",
    "    print(\"Test error = \"'{}'.format(test_error)+\" percent\"+\" in random forest\")\n",
    "\n",
    "    statdict['errper'].append(test_error)\n",
    "    statdict['names'].append('bag')\n",
    "    statdict['predictions'].append(y_bagging_test)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "# for i in range(4):\n",
    "#     statdicts[i] = do_baggingregr(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks and svr and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.31194243467\n",
      "0.0938775510204\n",
      "9.93221961946\n",
      "0.113265306122\n",
      "11.2447081488\n",
      "0.151020408163\n",
      "10.031814202\n",
      "0.161224489796\n",
      "11.5486219572\n",
      "0.167346938776\n",
      "11.4932168494\n",
      "0.172448979592\n"
     ]
    }
   ],
   "source": [
    "def do_ensemble(statdict, sets):\n",
    "    y_svr_test = statdict['predictions'][statdict['names'].index('SVM')]\n",
    "    y_forest_test = statdict['predictions'][statdict['names'].index('Rfor')]\n",
    "    y_extra_test = statdict['predictions'][statdict['names'].index('XRfor')]\n",
    "    ratings_test = sets['ratings_test']\n",
    "\n",
    "\n",
    "    prediction = [sum(x)/3 for x in zip(y_svr_test, y_forest_test, y_extra_test)]\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(len(prediction)):\n",
    "        if abs(prediction[i]-ratings_test[i])>1.0:\n",
    "            counter+= 1\n",
    "            #print(prediction[i], ratings_test[i])\n",
    "\n",
    "    error_ensemble = relative_error(prediction, ratings_test)\n",
    "\n",
    "    statdict['errper'].append(error_ensemble)\n",
    "    statdict['names'].append('ensemble')\n",
    "    statdict['predictions'].append(prediction)\n",
    "\n",
    "    print(error_ensemble)\n",
    "    print(counter/980.0)\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    statdicts[i] = do_ensemble(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(movie_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# movie_data['color'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.69\n",
      "Variance score: 0.37\n",
      "Mean squared error: 0.69\n",
      "Variance score: 0.38\n",
      "Mean squared error: 0.68\n",
      "Variance score: 0.39\n",
      "Mean squared error: 0.67\n",
      "Variance score: 0.39\n",
      "Mean squared error: 0.72\n",
      "Variance score: 0.41\n",
      "Mean squared error: 0.72\n",
      "Variance score: 0.41\n",
      "Mean squared error: 0.72\n",
      "Variance score: 0.41\n",
      "Mean squared error: 0.72\n",
      "Variance score: 0.41\n",
      "Mean squared error: 0.87\n",
      "Variance score: 0.34\n",
      "Mean squared error: 0.87\n",
      "Variance score: 0.33\n",
      "Mean squared error: 0.88\n",
      "Variance score: 0.33\n",
      "Mean squared error: 0.88\n",
      "Variance score: 0.33\n",
      "Mean squared error: 0.77\n",
      "Variance score: 0.38\n",
      "Mean squared error: 0.77\n",
      "Variance score: 0.38\n",
      "Mean squared error: 0.78\n",
      "Variance score: 0.38\n",
      "Mean squared error: 0.78\n",
      "Variance score: 0.38\n",
      "Mean squared error: 0.96\n",
      "Variance score: 0.31\n",
      "Mean squared error: 0.96\n",
      "Variance score: 0.31\n",
      "Mean squared error: 0.96\n",
      "Variance score: 0.31\n",
      "Mean squared error: 0.96\n",
      "Variance score: 0.31\n",
      "Mean squared error: 0.94\n",
      "Variance score: 0.31\n",
      "Mean squared error: 0.94\n",
      "Variance score: 0.32\n",
      "Mean squared error: 0.94\n",
      "Variance score: 0.32\n",
      "Mean squared error: 0.93\n",
      "Variance score: 0.32\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def do_linear_models(statdict, sets):\n",
    "    regr = linear_model.LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "\n",
    "    statdict = analyse_predictions(regr, sets, statdict)\n",
    "    statdict['names'].append('linreg')\n",
    "\n",
    "\n",
    "    regr = linear_model.Ridge()\n",
    "    statdict = analyse_predictions(regr, sets, statdict)\n",
    "    statdict['names'].append('ridgereg')\n",
    "\n",
    "\n",
    "    regr = linear_model.Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000000,\n",
    "       normalize=False, positive=False, precompute=False, random_state=None,\n",
    "       selection='cyclic', tol=0.0001, warm_start=False)\n",
    "    statdict = analyse_predictions(regr, sets, statdict)\n",
    "    statdict['names'].append('lassoreg')\n",
    "\n",
    "\n",
    "    regr = linear_model.BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
    "           fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
    "           normalize=False, tol=0.001, verbose=False)\n",
    "    statdict = analyse_predictions(regr, sets, statdict)\n",
    "    statdict['names'].append('bayesian')\n",
    "    \n",
    "    return statdict\n",
    "\n",
    "\n",
    "for i in range(NUM_THRESHOLDS):\n",
    "    statdicts[i] = do_linear_models(statdicts[i], sets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_threshold(statdicts):\n",
    "    n_groups = len(statdicts[0]['names'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5)\n",
    "    \n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.9 * (1.0 / len(statdicts))\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'k', 'y', 'w', 'b', 'g', 'r', 'c', 'm', 'k', 'y', 'w']\n",
    "    for i in range(len(statdicts)):\n",
    "        rects1 = plt.bar(index + bar_width * i, statdicts[i]['errper'], bar_width,\n",
    "                         alpha=opacity,\n",
    "                         color=colors[i],\n",
    "                         error_kw=error_config,\n",
    "                         label=\"threshold {}\".format(i + 1))\n",
    "    plt.xlabel('Algorithm')\n",
    "    plt.ylabel('Error Percentages (in %)')\n",
    "    plt.title('Error Percentages for various algorithms depending on the threshold')\n",
    "    plt.xticks(index + bar_width * 2, statdict['names'], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.legend(bbox_to_anchor=(0, 0.5), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    \n",
    "plot_threshold(statdicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_errors(statdict):\n",
    "    n_groups = len(statdict['names'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.9\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = plt.bar(index, statdict['errper'], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     error_kw=error_config)\n",
    "\n",
    "    plt.xlabel('Algorithm')\n",
    "    plt.ylabel('Error Percentages (in %)')\n",
    "    plt.title('Error Percentages for various algorithms using threshold 4')\n",
    "    plt.xticks(index + bar_width * 0.5, statdict['names'], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_errors(statdicts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_algorithm(statdicts, algorithm):\n",
    "    n_groups = NUM_THRESHOLDS\n",
    "\n",
    "    errpers = []\n",
    "    algind = statdicts[0]['names'].index(algorithm)\n",
    "    for t in range(n_groups):\n",
    "        errpers.append(statdicts[t]['errper'][algind])\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 1\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = plt.bar(index, errpers, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     error_kw=error_config)\n",
    "\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Error Percentages (in %)')\n",
    "    plt.title('Error Percentages for algorithm \\'{}\\' with varying thresholds'.format(algorithm))\n",
    "    plt.xticks(index + bar_width * 0.5, range(1, n_groups + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_algorithm(statdicts, 'ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
